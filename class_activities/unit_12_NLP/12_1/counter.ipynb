{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counter\n",
    "In this activity, you will create a function that preprocesses and outputs a list of the most common words in a corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package reuters to\n[nltk_data]     C:\\Users\\nospm\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package reuters is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\nospm\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\nospm\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\nospm\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "from nltk.corpus import reuters, stopwords\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Code to download corpora\n",
    "import nltk\n",
    "nltk.download('reuters')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'EGYPT AUTHORIZED TO BUY PL-480 WHEAT - USDA\\n  Egypt has been authorized to purchase\\n  about 200,000 tonnes of U.S. wheat under an existing PL-480\\n  agreement, the U.S. Agriculture Department said.\\n      It may buy the wheat, valued at 22.0 mln dlrs, between\\n  April 15 and August 31, 1987, and ship it from U.S. ports by\\n  September 30, the department said.\\n  \\n\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "# Corpus - list of articles about grains\n",
    "ids = reuters.fileids(categories='grain')\n",
    "corpus = [reuters.raw(i) for i in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocess function\n",
    "def process_text(doc):\n",
    "    sw = set(stopwords.words('english'))\n",
    "    regex = re.compile(\"[^a-zA-Z ]\")\n",
    "    re_clean = regex.sub('', doc)\n",
    "    words = word_tokenize(re_clean)\n",
    "    lem = [lemmatizer.lemmatize(word) for word in words]\n",
    "    output = [word.lower() for word in lem if word.lower() not in sw]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['china', 'daily', 'say', 'vermin', 'eat']"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "processed_list = process_text(corpus[0])\n",
    "processed_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the word_counter function\n",
    "def word_counter(list): \n",
    "    word_counts = Counter(list)\n",
    "    return dict(word_counts.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'china': 4,\n",
       " 'pct': 4,\n",
       " 'said': 3,\n",
       " 'daily': 2,\n",
       " 'vermin': 2,\n",
       " 'grain': 2,\n",
       " 'stock': 2,\n",
       " 'seven': 2,\n",
       " 'mln': 2,\n",
       " 'tonne': 2,\n",
       " 'paper': 2,\n",
       " 'waste': 2,\n",
       " 'storage': 2,\n",
       " 'preservation': 2,\n",
       " 'say': 1,\n",
       " 'eat': 1,\n",
       " 'survey': 1,\n",
       " 'province': 1,\n",
       " 'city': 1,\n",
       " 'showed': 1}"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "word_counter_results = word_counter(processed_list)\n",
    "word_counter_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bigram counter function\n",
    "def bigram_counter(corpus): \n",
    "    bigram_counts = Counter(ngrams(processed, n=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}